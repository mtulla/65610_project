{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model\n",
    "from mingpt.model import GPT\n",
    "from mingpt.utils import CfgNode as CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fhegpt.qmodel import QuantCasualSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CN(\n",
    "    n_embd = 768,\n",
    "    n_layer = 12,\n",
    "    n_head=12,\n",
    "    attn_pdrop=0.1,\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    block_size=1000\n",
    ")\n",
    "qcsa = QuantCasualSelfAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size torch.Size([1, 10, 768])\n",
      "k size torch.Size([1, 12, 10, 64])\n",
      "att size torch.Size([1, 12, 10, 10])\n",
      "type:<built-in method type of Tensor object at 0x7f3d81cdae30>\n",
      "tensor([[[[ 0.2559,  0.2585, -0.2983,  ..., -0.0581,  0.1248,  0.0931],\n",
      "          [ 0.0219, -0.4066,  0.0548,  ...,  0.0169,  0.1305,  0.3161],\n",
      "          [ 0.2296, -0.0576,  0.2825,  ..., -0.0998,  0.4411,  0.4366],\n",
      "          ...,\n",
      "          [ 0.0602, -0.2794, -0.2469,  ...,  0.0423, -0.0406, -0.1028],\n",
      "          [ 0.2798, -0.2366,  0.3201,  ...,  0.1176,  0.0356,  0.0857],\n",
      "          [-0.1273, -0.2888,  0.0885,  ..., -0.0872,  0.2691,  0.0861]],\n",
      "\n",
      "         [[ 0.1555,  0.0967, -0.1450,  ...,  0.0814, -0.2457,  0.1188],\n",
      "          [ 0.3947,  0.2900,  0.2468,  ..., -0.1729,  0.2216, -0.3068],\n",
      "          [-0.0235,  0.0289,  0.1055,  ..., -0.0768, -0.1076, -0.0668],\n",
      "          ...,\n",
      "          [ 0.0737,  0.2839,  0.1540,  ...,  0.0555, -0.0708, -0.0437],\n",
      "          [-0.1488,  0.2233, -0.2198,  ...,  0.1026, -0.0734,  0.0634],\n",
      "          [ 0.0434,  0.1802,  0.2865,  ...,  0.0466,  0.0461, -0.1983]],\n",
      "\n",
      "         [[ 0.0346, -0.0520,  0.0392,  ..., -0.2216,  0.1671,  0.1665],\n",
      "          [ 0.2049, -0.0065, -0.2169,  ..., -0.2025, -0.1046,  0.1568],\n",
      "          [-0.0651,  0.0724,  0.1348,  ...,  0.3575, -0.0468,  0.0078],\n",
      "          ...,\n",
      "          [-0.0023,  0.2599, -0.0671,  ..., -0.0978, -0.0758, -0.1108],\n",
      "          [ 0.1442, -0.0431,  0.1002,  ..., -0.4052,  0.2691,  0.4856],\n",
      "          [-0.1587, -0.2416, -0.0766,  ..., -0.1723,  0.0751,  0.1089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0595,  0.0676, -0.1180,  ..., -0.2009,  0.0690, -0.0837],\n",
      "          [-0.0731,  0.0447,  0.2053,  ..., -0.1760,  0.2517, -0.3124],\n",
      "          [-0.2750, -0.0815,  0.0772,  ...,  0.0983, -0.1846,  0.0311],\n",
      "          ...,\n",
      "          [ 0.4343,  0.2407,  0.1117,  ...,  0.0337, -0.1459, -0.2671],\n",
      "          [ 0.0469,  0.1958, -0.0647,  ..., -0.0711,  0.0537, -0.1097],\n",
      "          [-0.1008,  0.1043,  0.1180,  ...,  0.2202,  0.0141,  0.0241]],\n",
      "\n",
      "         [[ 0.0119,  0.0117, -0.0483,  ...,  0.0746, -0.2268, -0.1759],\n",
      "          [ 0.1805, -0.2279, -0.0870,  ...,  0.1845,  0.0531, -0.1662],\n",
      "          [ 0.1117, -0.0161,  0.2824,  ..., -0.3298,  0.0583,  0.4588],\n",
      "          ...,\n",
      "          [ 0.1440, -0.1991, -0.0236,  ...,  0.1082,  0.0518,  0.1934],\n",
      "          [ 0.0828, -0.0593, -0.2567,  ...,  0.0513,  0.2626,  0.0813],\n",
      "          [ 0.0078, -0.0577, -0.1167,  ..., -0.0299,  0.4447,  0.1205]],\n",
      "\n",
      "         [[ 0.0586, -0.0928, -0.0020,  ..., -0.1148, -0.0107,  0.0789],\n",
      "          [-0.0742, -0.2672, -0.0735,  ...,  0.0633, -0.0113,  0.0851],\n",
      "          [ 0.0538,  0.0857,  0.0618,  ..., -0.2317, -0.1065, -0.1991],\n",
      "          ...,\n",
      "          [-0.1213, -0.2063,  0.1696,  ..., -0.0144, -0.0700,  0.1559],\n",
      "          [-0.3957, -0.2484,  0.0342,  ..., -0.3958, -0.1436,  0.1428],\n",
      "          [-0.0262, -0.1544,  0.2837,  ...,  0.0647, -0.1233,  0.1702]]]])\n",
      "x size torch.Size([1, 10, 768])\n",
      "k size torch.Size([1, 12, 10, 64])\n",
      "att size torch.Size([1, 12, 10, 10])\n",
      "type:<built-in method type of Tensor object at 0x7f3d81b5ade0>\n",
      "tensor([[[[ 0.2559,  0.2585, -0.2983,  ..., -0.0581,  0.1248,  0.0931],\n",
      "          [ 0.0219, -0.4066,  0.0548,  ...,  0.0169,  0.1305,  0.3161],\n",
      "          [ 0.2296, -0.0576,  0.2825,  ..., -0.0998,  0.4411,  0.4366],\n",
      "          ...,\n",
      "          [ 0.0602, -0.2794, -0.2469,  ...,  0.0423, -0.0406, -0.1028],\n",
      "          [ 0.2798, -0.2366,  0.3201,  ...,  0.1176,  0.0356,  0.0857],\n",
      "          [-0.1273, -0.2888,  0.0885,  ..., -0.0872,  0.2691,  0.0861]],\n",
      "\n",
      "         [[ 0.1555,  0.0967, -0.1450,  ...,  0.0814, -0.2457,  0.1188],\n",
      "          [ 0.3947,  0.2900,  0.2468,  ..., -0.1729,  0.2216, -0.3068],\n",
      "          [-0.0235,  0.0289,  0.1055,  ..., -0.0768, -0.1076, -0.0668],\n",
      "          ...,\n",
      "          [ 0.0737,  0.2839,  0.1540,  ...,  0.0555, -0.0708, -0.0437],\n",
      "          [-0.1488,  0.2233, -0.2198,  ...,  0.1026, -0.0734,  0.0634],\n",
      "          [ 0.0434,  0.1802,  0.2865,  ...,  0.0466,  0.0461, -0.1983]],\n",
      "\n",
      "         [[ 0.0346, -0.0520,  0.0392,  ..., -0.2216,  0.1671,  0.1665],\n",
      "          [ 0.2049, -0.0065, -0.2169,  ..., -0.2025, -0.1046,  0.1568],\n",
      "          [-0.0651,  0.0724,  0.1348,  ...,  0.3575, -0.0468,  0.0078],\n",
      "          ...,\n",
      "          [-0.0023,  0.2599, -0.0671,  ..., -0.0978, -0.0758, -0.1108],\n",
      "          [ 0.1442, -0.0431,  0.1002,  ..., -0.4052,  0.2691,  0.4856],\n",
      "          [-0.1587, -0.2416, -0.0766,  ..., -0.1723,  0.0751,  0.1089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0595,  0.0676, -0.1180,  ..., -0.2009,  0.0690, -0.0837],\n",
      "          [-0.0731,  0.0447,  0.2053,  ..., -0.1760,  0.2517, -0.3124],\n",
      "          [-0.2750, -0.0815,  0.0772,  ...,  0.0983, -0.1846,  0.0311],\n",
      "          ...,\n",
      "          [ 0.4343,  0.2407,  0.1117,  ...,  0.0337, -0.1459, -0.2671],\n",
      "          [ 0.0469,  0.1958, -0.0647,  ..., -0.0711,  0.0537, -0.1097],\n",
      "          [-0.1008,  0.1043,  0.1180,  ...,  0.2202,  0.0141,  0.0241]],\n",
      "\n",
      "         [[ 0.0119,  0.0117, -0.0483,  ...,  0.0746, -0.2268, -0.1759],\n",
      "          [ 0.1805, -0.2279, -0.0870,  ...,  0.1845,  0.0531, -0.1662],\n",
      "          [ 0.1117, -0.0161,  0.2824,  ..., -0.3298,  0.0583,  0.4588],\n",
      "          ...,\n",
      "          [ 0.1440, -0.1991, -0.0236,  ...,  0.1082,  0.0518,  0.1934],\n",
      "          [ 0.0828, -0.0593, -0.2567,  ...,  0.0513,  0.2626,  0.0813],\n",
      "          [ 0.0078, -0.0577, -0.1167,  ..., -0.0299,  0.4447,  0.1205]],\n",
      "\n",
      "         [[ 0.0586, -0.0928, -0.0020,  ..., -0.1148, -0.0107,  0.0789],\n",
      "          [-0.0742, -0.2672, -0.0735,  ...,  0.0633, -0.0113,  0.0851],\n",
      "          [ 0.0538,  0.0857,  0.0618,  ..., -0.2317, -0.1065, -0.1991],\n",
      "          ...,\n",
      "          [-0.1213, -0.2063,  0.1696,  ..., -0.0144, -0.0700,  0.1559],\n",
      "          [-0.3957, -0.2484,  0.0342,  ..., -0.3958, -0.1436,  0.1428],\n",
      "          [-0.0262, -0.1544,  0.2837,  ...,  0.0647, -0.1233,  0.1702]]]])\n",
      "quantized op <concrete.ml.quantization.quantized_ops.QuantizedBrevitasQuant object at 0x7f3d81c26230>\n",
      "optype <bound method QuantizedOp.op_type of <class 'concrete.ml.quantization.quantized_ops.QuantizedBrevitasQuant'>>\n",
      "quantized op <concrete.ml.quantization.quantized_ops.QuantizedGemm object at 0x7f3d81c25ae0>\n",
      "optype <bound method QuantizedOp.op_type of <class 'concrete.ml.quantization.quantized_ops.QuantizedGemm'>>\n",
      "quantized op <concrete.ml.quantization.quantized_ops.QuantizedGemm object at 0x7f3d81c255a0>\n",
      "optype <bound method QuantizedOp.op_type of <class 'concrete.ml.quantization.quantized_ops.QuantizedGemm'>>\n",
      "quantized op <concrete.ml.quantization.quantized_ops.QuantizedGemm object at 0x7f3d81c3c160>\n",
      "optype <bound method QuantizedOp.op_type of <class 'concrete.ml.quantization.quantized_ops.QuantizedGemm'>>\n",
      "quantized op <concrete.ml.quantization.quantized_ops.QuantizedReshape object at 0x7f3d81c3c5e0>\n",
      "optype <bound method QuantizedOp.op_type of <class 'concrete.ml.quantization.quantized_ops.QuantizedReshape'>>\n",
      "quantized op <concrete.ml.quantization.quantized_ops.QuantizedReshape object at 0x7f3d81c3caf0>\n",
      "optype <bound method QuantizedOp.op_type of <class 'concrete.ml.quantization.quantized_ops.QuantizedReshape'>>\n",
      "quantized op <concrete.ml.quantization.quantized_ops.QuantizedTranspose object at 0x7f3d81c3cbb0>\n",
      "optype <bound method QuantizedOp.op_type of <class 'concrete.ml.quantization.quantized_ops.QuantizedTranspose'>>\n",
      "quantized op <concrete.ml.quantization.quantized_ops.QuantizedReshape object at 0x7f3d81c3d9f0>\n",
      "optype <bound method QuantizedOp.op_type of <class 'concrete.ml.quantization.quantized_ops.QuantizedReshape'>>\n",
      "quantized op <concrete.ml.quantization.quantized_ops.QuantizedTranspose object at 0x7f3d81c3dcc0>\n",
      "optype <bound method QuantizedOp.op_type of <class 'concrete.ml.quantization.quantized_ops.QuantizedTranspose'>>\n",
      "quantized op <concrete.ml.quantization.quantized_ops.QuantizedTranspose object at 0x7f3d81c3ded0>\n",
      "optype <bound method QuantizedOp.op_type of <class 'concrete.ml.quantization.quantized_ops.QuantizedTranspose'>>\n",
      "quantized op <concrete.ml.quantization.quantized_ops.QuantizedMatMul object at 0x7f3d81c3cb20>\n",
      "optype <bound method QuantizedOp.op_type of <class 'concrete.ml.quantization.quantized_ops.QuantizedMatMul'>>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Unsupported dimension for the weight input of the gemm: 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, config\u001b[38;5;241m.\u001b[39mn_embd)\n\u001b[0;32m----> 2\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_brevitas_qat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqcsa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/concrete-ml/src/concrete/ml/torch/compile.py:534\u001b[0m, in \u001b[0;36mcompile_brevitas_qat_model\u001b[0;34m(torch_model, torch_inputset, n_bits, configuration, artifacts, show_mlir, rounding_threshold_bits, p_error, global_p_error, output_onnx_file, verbose, inputs_encryption_status, reduce_sum_copy)\u001b[0m\n\u001b[1;32m    528\u001b[0m assert_true(\n\u001b[1;32m    529\u001b[0m     n_bits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n_bits, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mdict\u001b[39m)),\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe n_bits parameter must be either a dictionary, an integer or None\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    531\u001b[0m )\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# Compile using the ONNX conversion flow, in QAT mode\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m q_module \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_onnx_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_inputset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimport_qat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_mlir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_mlir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_p_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_p_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_encryption_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_encryption_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# Remove the tempfile if we used one\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_tempfile:\n",
      "File \u001b[0;32m/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/concrete-ml/src/concrete/ml/torch/compile.py:383\u001b[0m, in \u001b[0;36mcompile_onnx_model\u001b[0;34m(onnx_model, torch_inputset, import_qat, configuration, artifacts, show_mlir, n_bits, rounding_threshold_bits, p_error, global_p_error, verbose, inputs_encryption_status, reduce_sum_copy)\u001b[0m\n\u001b[1;32m    376\u001b[0m onnx_model_opset_version \u001b[38;5;241m=\u001b[39m get_onnx_opset_version(onnx_model)\n\u001b[1;32m    377\u001b[0m assert_true(\n\u001b[1;32m    378\u001b[0m     onnx_model_opset_version \u001b[38;5;241m==\u001b[39m OPSET_VERSION_FOR_ONNX_EXPORT,\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNX version must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOPSET_VERSION_FOR_ONNX_EXPORT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut it is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx_model_opset_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    381\u001b[0m )\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_torch_or_onnx_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_inputset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimport_qat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_mlir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_mlir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_p_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_p_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_encryption_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_encryption_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/concrete-ml/src/concrete/ml/torch/compile.py:195\u001b[0m, in \u001b[0;36m_compile_torch_or_onnx_model\u001b[0;34m(model, torch_inputset, import_qat, configuration, artifacts, show_mlir, n_bits, rounding_threshold_bits, p_error, global_p_error, verbose, inputs_encryption_status, reduce_sum_copy)\u001b[0m\n\u001b[1;32m    190\u001b[0m inputset_as_numpy_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    191\u001b[0m     convert_torch_tensor_or_numpy_array_to_numpy_array(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m to_tuple(torch_inputset)\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Build the quantized module\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m quantized_module \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_quantized_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_inputset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputset_as_numpy_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimport_qat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimport_qat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Check that p_error or global_p_error is not set in both the configuration and in the direct\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# parameters\u001b[39;00m\n\u001b[1;32m    206\u001b[0m check_there_is_no_p_error_options_in_configuration(configuration)\n",
      "File \u001b[0;32m/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/concrete-ml/src/concrete/ml/torch/compile.py:127\u001b[0m, in \u001b[0;36mbuild_quantized_module\u001b[0;34m(model, torch_inputset, import_qat, n_bits, rounding_threshold_bits, reduce_sum_copy)\u001b[0m\n\u001b[1;32m    121\u001b[0m post_training_quant \u001b[38;5;241m=\u001b[39m post_training(n_bits, numpy_model, rounding_threshold_bits)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Build the quantized module\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# FIXME: mismatch here. We traced with dummy_input_for_tracing which made some operator\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# only work over shape of (1, ., .). For example, some reshape have newshape hardcoded based\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# on the inputset we sent in the NumpyModule.\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m quantized_module \u001b[38;5;241m=\u001b[39m \u001b[43mpost_training_quant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputset_as_numpy_tuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/4127\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_sum_copy:\n",
      "File \u001b[0;32m/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/concrete-ml/src/concrete/ml/quantization/post_training.py:702\u001b[0m, in \u001b[0;36mONNXConverter.quantize_module\u001b[0;34m(self, *calibration_data)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;66;03m# First transform all parameters to their quantized version\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_quantize_params()\n\u001b[0;32m--> 702\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quantize_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcalibration_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# Create quantized module from self.quant_layers_dict\u001b[39;00m\n\u001b[1;32m    705\u001b[0m quantized_module \u001b[38;5;241m=\u001b[39m QuantizedModule(\n\u001b[1;32m    706\u001b[0m     ordered_module_input_names\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    707\u001b[0m         graph_input\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m graph_input \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_model\u001b[38;5;241m.\u001b[39monnx_model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minput\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    713\u001b[0m     onnx_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_model\u001b[38;5;241m.\u001b[39monnx_model,\n\u001b[1;32m    714\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/concrete-ml/src/concrete/ml/quantization/post_training.py:629\u001b[0m, in \u001b[0;36mONNXConverter._quantize_layers\u001b[0;34m(self, *input_calibration_data)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_ops_dict[output_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(variable_input_names),\n\u001b[1;32m    622\u001b[0m     quantized_op_instance,\n\u001b[1;32m    623\u001b[0m )\n\u001b[1;32m    625\u001b[0m layer_quant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    626\u001b[0m     node_override_quantizer\u001b[38;5;241m.\u001b[39mget(input_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m variable_input_names\n\u001b[1;32m    628\u001b[0m )\n\u001b[0;32m--> 629\u001b[0m output_calibration_data, layer_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantized_op_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcurr_calibration_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_quant\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m node_results[output_name] \u001b[38;5;241m=\u001b[39m output_calibration_data\n\u001b[1;32m    633\u001b[0m node_override_quantizer[output_name] \u001b[38;5;241m=\u001b[39m layer_quantizer\n",
      "File \u001b[0;32m/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/concrete-ml/src/concrete/ml/quantization/post_training.py:1004\u001b[0m, in \u001b[0;36mPostTrainingQATImporter._process_layer\u001b[0;34m(self, quantized_op, quantizers, *calibration_data)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_layer\u001b[39m(\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    985\u001b[0m     quantized_op: QuantizedOp,\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;241m*\u001b[39mcalibration_data: numpy\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    987\u001b[0m     quantizers: List[Optional[UniformQuantizer]],\n\u001b[1;32m    988\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[numpy\u001b[38;5;241m.\u001b[39mndarray, Optional[UniformQuantizer]]:\n\u001b[1;32m    989\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Configure a graph operation by calibrating it for Quantization Aware Training.\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \n\u001b[1;32m    991\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;124;03m        numpy.ndarray: calibration data for the following operators\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calibrate_layers_activation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantized_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcalibration_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantizers\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/concrete-ml/src/concrete/ml/quantization/post_training.py:373\u001b[0m, in \u001b[0;36mONNXConverter._calibrate_layers_activation\u001b[0;34m(self, calibrate_quantized, quantized_op, quantizers, *calibration_data)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized op\u001b[39m\u001b[38;5;124m\"\u001b[39m, quantized_op)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptype\u001b[39m\u001b[38;5;124m\"\u001b[39m, quantized_op\u001b[38;5;241m.\u001b[39mop_type)\n\u001b[0;32m--> 373\u001b[0m quant_result \u001b[38;5;241m=\u001b[39m \u001b[43mquantized_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mq_calibration_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mq_impl_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quantized_op\u001b[38;5;241m.\u001b[39mproduces_graph_output:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quant_result, QuantizedArray), (\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe PyTorch module can not return a raw value, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuch as a clear constant or the shape of a tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    379\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/concrete-ml/src/concrete/ml/quantization/quantized_ops.py:217\u001b[0m, in \u001b[0;36mQuantizedGemm.q_impl\u001b[0;34m(self, calibrate_rounding, *q_inputs, **attrs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     input1_q_values \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    211\u001b[0m         numpy\u001b[38;5;241m.\u001b[39mtranspose(q_input1\u001b[38;5;241m.\u001b[39mqvalues) \u001b[38;5;28;01mif\u001b[39;00m transpose_inputs1 \u001b[38;5;28;01melse\u001b[39;00m q_input1\u001b[38;5;241m.\u001b[39mqvalues\n\u001b[1;32m    212\u001b[0m     )\n\u001b[1;32m    213\u001b[0m input2_q_values \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    214\u001b[0m     numpy\u001b[38;5;241m.\u001b[39mtranspose(q_input2\u001b[38;5;241m.\u001b[39mqvalues) \u001b[38;5;28;01mif\u001b[39;00m transpose_inputs2 \u001b[38;5;28;01melse\u001b[39;00m q_input2\u001b[38;5;241m.\u001b[39mqvalues\n\u001b[1;32m    215\u001b[0m )\n\u001b[0;32m--> 217\u001b[0m \u001b[43massert_true\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput2_q_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUnsupported dimension for the weight input of the gemm: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minput2_q_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# For mypy\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_quant_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/concrete-ml/src/concrete/ml/common/debugging/custom_assert.py:41\u001b[0m, in \u001b[0;36massert_true\u001b[0;34m(condition, on_error_msg, error_type)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_true\u001b[39m(\n\u001b[1;32m     30\u001b[0m     condition: \u001b[38;5;28mbool\u001b[39m, on_error_msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, error_type: Type[\u001b[38;5;167;01mException\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mAssertionError\u001b[39;00m\n\u001b[1;32m     31\u001b[0m ):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Provide a custom assert to check that the condition is True.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[43m_custom_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_error_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/kingo/Desktop/MIT2023-24/6.5610/65610_project/concrete-ml/src/concrete/ml/common/debugging/custom_assert.py:26\u001b[0m, in \u001b[0;36m_custom_assert\u001b[0;34m(condition, on_error_msg, error_type)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Provide a custom assert which is kept even if the optimized Python mode is used.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mSee https://docs.python.org/3/reference/simple_stmts.html#assert for the documentation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m condition:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_type(on_error_msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Unsupported dimension for the weight input of the gemm: 4"
     ]
    }
   ],
   "source": [
    "torch_input = torch.randn(1, 10, config.n_embd)\n",
    "quantized_model = compile_brevitas_qat_model(qcsa, torch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
