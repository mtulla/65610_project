{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mlizardi/65610_project/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/pkg/cuda/cuda-11.8'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model\n",
    "from mingpt.model import GPT\n",
    "from mingpt.utils import CfgNode as CN\n",
    "\n",
    "from fhegpt.qmodel import QuantCasualSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CN(\n",
    "    n_embd = 768,\n",
    "    n_layer = 12,\n",
    "    n_head=12,\n",
    "    attn_pdrop=0.1,\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    block_size=1000\n",
    ")\n",
    "qcsa = QuantCasualSelfAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following ONNX operators are required to convert the torch model to numpy but are not currently implemented: Softmax, Split.\nAvailable ONNX operators: Abs, Acos, Acosh, Add, Asin, Asinh, Atan, Atanh, AveragePool, BatchNormalization, Cast, Celu, Clip, Concat, Constant, ConstantOfShape, Conv, Cos, Cosh, Div, Elu, Equal, Erf, Exp, Expand, Flatten, Floor, Gather, Gemm, Greater, GreaterOrEqual, HardSigmoid, HardSwish, Identity, LeakyRelu, Less, LessOrEqual, Log, MatMul, Max, MaxPool, Min, Mul, Neg, Not, Or, PRelu, Pad, Pow, ReduceSum, Relu, Reshape, Round, Selu, Shape, Sigmoid, Sign, Sin, Sinh, Slice, Softplus, Squeeze, Sub, Tan, Tanh, ThresholdedRelu, Transpose, Unfold, Unsqueeze, Where, onnx.brevitas.Quant",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, config\u001b[38;5;241m.\u001b[39mn_embd)\n\u001b[0;32m----> 2\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_brevitas_qat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqcsa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/65610_project/concrete-ml/src/concrete/ml/torch/compile.py:534\u001b[0m, in \u001b[0;36mcompile_brevitas_qat_model\u001b[0;34m(torch_model, torch_inputset, n_bits, configuration, artifacts, show_mlir, rounding_threshold_bits, p_error, global_p_error, output_onnx_file, verbose, inputs_encryption_status, reduce_sum_copy)\u001b[0m\n\u001b[1;32m    528\u001b[0m assert_true(\n\u001b[1;32m    529\u001b[0m     n_bits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n_bits, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mdict\u001b[39m)),\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe n_bits parameter must be either a dictionary, an integer or None\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    531\u001b[0m )\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# Compile using the ONNX conversion flow, in QAT mode\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m q_module \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_onnx_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_inputset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimport_qat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_mlir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_mlir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_p_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_p_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_encryption_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_encryption_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# Remove the tempfile if we used one\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_tempfile:\n",
      "File \u001b[0;32m~/65610_project/concrete-ml/src/concrete/ml/torch/compile.py:383\u001b[0m, in \u001b[0;36mcompile_onnx_model\u001b[0;34m(onnx_model, torch_inputset, import_qat, configuration, artifacts, show_mlir, n_bits, rounding_threshold_bits, p_error, global_p_error, verbose, inputs_encryption_status, reduce_sum_copy)\u001b[0m\n\u001b[1;32m    376\u001b[0m onnx_model_opset_version \u001b[38;5;241m=\u001b[39m get_onnx_opset_version(onnx_model)\n\u001b[1;32m    377\u001b[0m assert_true(\n\u001b[1;32m    378\u001b[0m     onnx_model_opset_version \u001b[38;5;241m==\u001b[39m OPSET_VERSION_FOR_ONNX_EXPORT,\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNX version must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOPSET_VERSION_FOR_ONNX_EXPORT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut it is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx_model_opset_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    381\u001b[0m )\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_torch_or_onnx_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_inputset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimport_qat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_mlir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_mlir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_p_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_p_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_encryption_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_encryption_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/65610_project/concrete-ml/src/concrete/ml/torch/compile.py:195\u001b[0m, in \u001b[0;36m_compile_torch_or_onnx_model\u001b[0;34m(model, torch_inputset, import_qat, configuration, artifacts, show_mlir, n_bits, rounding_threshold_bits, p_error, global_p_error, verbose, inputs_encryption_status, reduce_sum_copy)\u001b[0m\n\u001b[1;32m    190\u001b[0m inputset_as_numpy_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    191\u001b[0m     convert_torch_tensor_or_numpy_array_to_numpy_array(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m to_tuple(torch_inputset)\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Build the quantized module\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m quantized_module \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_quantized_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_inputset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputset_as_numpy_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimport_qat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimport_qat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_sum_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Check that p_error or global_p_error is not set in both the configuration and in the direct\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# parameters\u001b[39;00m\n\u001b[1;32m    206\u001b[0m check_there_is_no_p_error_options_in_configuration(configuration)\n",
      "File \u001b[0;32m~/65610_project/concrete-ml/src/concrete/ml/torch/compile.py:117\u001b[0m, in \u001b[0;36mbuild_quantized_module\u001b[0;34m(model, torch_inputset, import_qat, n_bits, rounding_threshold_bits, reduce_sum_copy)\u001b[0m\n\u001b[1;32m    112\u001b[0m dummy_input_for_tracing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    113\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfrom_numpy(val[[\u001b[38;5;241m0\u001b[39m], ::])\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m inputset_as_numpy_tuple\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Create corresponding numpy model\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m numpy_model \u001b[38;5;241m=\u001b[39m \u001b[43mNumpyModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_input_for_tracing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Quantize with post-training static method, to have a model with integer weights\u001b[39;00m\n\u001b[1;32m    120\u001b[0m post_training \u001b[38;5;241m=\u001b[39m PostTrainingQATImporter \u001b[38;5;28;01mif\u001b[39;00m import_qat \u001b[38;5;28;01melse\u001b[39;00m PostTrainingAffineQuantization\n",
      "File \u001b[0;32m~/65610_project/concrete-ml/src/concrete/ml/torch/numpy_module.py:59\u001b[0m, in \u001b[0;36mNumpyModule.__init__\u001b[0;34m(self, model, dummy_input, debug_onnx_output_file_path)\u001b[0m\n\u001b[1;32m     52\u001b[0m     onnx_model_opset_version \u001b[38;5;241m=\u001b[39m get_onnx_opset_version(model)\n\u001b[1;32m     53\u001b[0m     assert_true(\n\u001b[1;32m     54\u001b[0m         onnx_model_opset_version \u001b[38;5;241m==\u001b[39m OPSET_VERSION_FOR_ONNX_EXPORT,\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNX version must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOPSET_VERSION_FOR_ONNX_EXPORT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut it is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx_model_opset_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_onnx_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_equivalent_numpy_forward_from_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel must be a torch.nn.Module or an onnx.ModelProto, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m     )\n",
      "File \u001b[0;32m~/65610_project/concrete-ml/src/concrete/ml/onnx/convert.py:257\u001b[0m, in \u001b[0;36mget_equivalent_numpy_forward_from_onnx\u001b[0;34m(onnx_model, check_model)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_equivalent_numpy_forward_from_onnx\u001b[39m(\n\u001b[1;32m    241\u001b[0m     onnx_model: onnx\u001b[38;5;241m.\u001b[39mModelProto,\n\u001b[1;32m    242\u001b[0m     check_model: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    243\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Tuple[numpy\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]], onnx\u001b[38;5;241m.\u001b[39mModelProto]:\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the numpy equivalent forward of the provided ONNX model.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            the equivalent numpy function.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     equivalent_onnx_model \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_onnx_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Return lambda of numpy equivalent of onnx execution\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs: execute_onnx_with_numpy(equivalent_onnx_model\u001b[38;5;241m.\u001b[39mgraph, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    262\u001b[0m     ), equivalent_onnx_model\n",
      "File \u001b[0;32m~/65610_project/concrete-ml/src/concrete/ml/onnx/convert.py:231\u001b[0m, in \u001b[0;36mpreprocess_onnx_model\u001b[0;34m(onnx_model, check_model)\u001b[0m\n\u001b[1;32m    229\u001b[0m unsupported_operators \u001b[38;5;241m=\u001b[39m required_onnx_operators \u001b[38;5;241m-\u001b[39m IMPLEMENTED_ONNX_OPS\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unsupported_operators) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following ONNX operators are required to convert the torch model to numpy but are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot currently implemented: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28msorted\u001b[39m(unsupported_operators))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable ONNX operators: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28msorted\u001b[39m(IMPLEMENTED_ONNX_OPS))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m equivalent_onnx_model\n",
      "\u001b[0;31mValueError\u001b[0m: The following ONNX operators are required to convert the torch model to numpy but are not currently implemented: Softmax, Split.\nAvailable ONNX operators: Abs, Acos, Acosh, Add, Asin, Asinh, Atan, Atanh, AveragePool, BatchNormalization, Cast, Celu, Clip, Concat, Constant, ConstantOfShape, Conv, Cos, Cosh, Div, Elu, Equal, Erf, Exp, Expand, Flatten, Floor, Gather, Gemm, Greater, GreaterOrEqual, HardSigmoid, HardSwish, Identity, LeakyRelu, Less, LessOrEqual, Log, MatMul, Max, MaxPool, Min, Mul, Neg, Not, Or, PRelu, Pad, Pow, ReduceSum, Relu, Reshape, Round, Selu, Shape, Sigmoid, Sign, Sin, Sinh, Slice, Softplus, Squeeze, Sub, Tan, Tanh, ThresholdedRelu, Transpose, Unfold, Unsqueeze, Where, onnx.brevitas.Quant"
     ]
    }
   ],
   "source": [
    "torch_input = torch.randn(1, 10, config.n_embd)\n",
    "quantized_model = compile_brevitas_qat_model(qcsa, torch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
