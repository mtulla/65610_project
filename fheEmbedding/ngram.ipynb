{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117ec2690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.ao.quantization\n",
    "import torch.ao.nn\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.\n",
    "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "# Print the first 3, just so you can see what they look like.\n",
    "print(ngrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantNGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(QuantNGramLanguageModeler, self).__init__()\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.embeddings = torch.ao.nn.quantized.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        qinputs = self.quant(inputs)\n",
    "        embeds = self.embeddings(qinputs.int()).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds.float()))\n",
    "        out = self.linear2(out)\n",
    "        out = self.dequant(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model):\n",
    "    losses = []\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0\n",
    "        for context, target in ngrams:\n",
    "\n",
    "            # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "            # into integer indices and wrap them in tensors)\n",
    "            context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.float)\n",
    "\n",
    "            # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "            # new instance, you need to zero out the gradients from the old\n",
    "            # instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 3. Run the forward pass, getting log probabilities over next\n",
    "            # words\n",
    "            log_probs = model(context_idxs)\n",
    "\n",
    "            # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "            # word wrapped in a tensor)\n",
    "            loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "            # Step 5. Do the backward pass and update the gradient\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "            total_loss += loss.item()\n",
    "        losses.append(total_loss)\n",
    "    print(losses)  # The loss decreased every iteration over the training data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Miguel/Desktop/MIT/spring2024/6.5610/65610_project/venv_embeddings/lib/python3.10/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = QuantNGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
    "model_prepared = torch.ao.quantization.prepare_qat(model.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4251.218174934387, 2979.6950171366334, 2231.616878427565, 1574.5523364543915, 1174.4323370456696, 753.2356539964676, 598.8750112201087, 507.3967487961054, 435.8005963098258, 384.04318079166114]\n"
     ]
    }
   ],
   "source": [
    "training_loop(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 96.,  43., 224.,  35.,   1.,   0.,   0.,   0., 111.,   4.],\n",
       "       size=(10,), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=1.0, zero_point=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prepared.embeddings.weight()[word_to_ix[\"beauty\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find engine for operation quantized::linear_prepack NoQEngine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m model_prepared\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;32m----> 2\u001b[0m quant_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mao\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepared\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/MIT/spring2024/6.5610/65610_project/venv_embeddings/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:553\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[0m\n",
      "\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n",
      "\u001b[1;32m    552\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n",
      "\u001b[0;32m--> 553\u001b[0m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n",
      "\u001b[1;32m    557\u001b[0m     _remove_qconfig(module)\n",
      "\n",
      "File \u001b[0;32m~/Desktop/MIT/spring2024/6.5610/65610_project/venv_embeddings/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:593\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n",
      "\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n",
      "\u001b[1;32m    590\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n",
      "\u001b[1;32m    591\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n",
      "\u001b[1;32m    592\u001b[0m                  is_reference, convert_custom_config_dict)\n",
      "\u001b[0;32m--> 593\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;32m    596\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "\n",
      "File \u001b[0;32m~/Desktop/MIT/spring2024/6.5610/65610_project/venv_embeddings/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:626\u001b[0m, in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping)\u001b[0m\n",
      "\u001b[1;32m    624\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod, weight_qparams)\n",
      "\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 626\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m \u001b[43mqmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    627\u001b[0m     swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped:\n",
      "\u001b[1;32m    630\u001b[0m     \u001b[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/MIT/spring2024/6.5610/65610_project/venv_embeddings/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/linear.py:277\u001b[0m, in \u001b[0;36mLinear.from_float\u001b[0;34m(cls, mod)\u001b[0m\n",
      "\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeight observer must have dtype torch.qint8\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;32m    276\u001b[0m qweight \u001b[38;5;241m=\u001b[39m _quantize_weight(mod\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat(), weight_post_process)\n",
      "\u001b[0;32m--> 277\u001b[0m qlinear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    278\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    279\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    280\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, mod\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;32m    281\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(act_scale)\n",
      "\n",
      "File \u001b[0;32m~/Desktop/MIT/spring2024/6.5610/65610_project/venv_embeddings/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/linear.py:151\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias_, dtype)\u001b[0m\n",
      "\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported dtype specified for quantized Linear!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m \u001b[43mLinearPackedParams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, bias)\n",
      "\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/MIT/spring2024/6.5610/65610_project/venv_embeddings/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/linear.py:27\u001b[0m, in \u001b[0;36mLinearPackedParams.__init__\u001b[0;34m(self, dtype)\u001b[0m\n",
      "\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "\u001b[1;32m     26\u001b[0m     wq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/MIT/spring2024/6.5610/65610_project/venv_embeddings/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/linear.py:32\u001b[0m, in \u001b[0;36mLinearPackedParams.set_weight_bias\u001b[0;34m(self, weight, bias)\u001b[0m\n",
      "\u001b[1;32m     29\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n",
      "\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: torch\u001b[38;5;241m.\u001b[39mTensor, bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8:\n",
      "\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_prepack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_prepack_fp16(weight, bias)\n",
      "\n",
      "File \u001b[0;32m~/Desktop/MIT/spring2024/6.5610/65610_project/venv_embeddings/lib/python3.10/site-packages/torch/_ops.py:854\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n",
      "\u001b[1;32m    848\u001b[0m     \u001b[38;5;66;03m# named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n",
      "\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n",
      "\u001b[0;32m--> 854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Didn't find engine for operation quantized::linear_prepack NoQEngine"
     ]
    }
   ],
   "source": [
    "model_prepared.eval()\n",
    "quant_model = torch.ao.quantization.convert(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
